{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from transformers import *\nimport torch\nimport matplotlib.pyplot as plt\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df  = df[['text', 'target']]\ntest = test[['text']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport nltk\nfrom bs4 import BeautifulSoup\nfrom nltk.tokenize import word_tokenize\nimport nltk\nnltk.download('punkt')\nnltk.download('wordnet')\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\ndef string_process(input):\n    #remove html content\n    review_text = BeautifulSoup(input).get_text()\n    #remove non-alphabetic characters\n    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n    #tokenize the sentences\n    words = word_tokenize(review_text.lower())\n    \n    #lemmatize each word to its lemma\n    lemma_words = [lemmatizer.lemmatize(i) for i in words]\n    \n    return \" \".join(lemma_words)\n\ndf['text'] = df['text'].apply(string_process)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['text'] = test['text'].apply(string_process)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['target'].hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lenstr(t):\n    return len(t.split())\ndf['len'] = df['text'].apply(lenstr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['len'] = test['text'].apply(lenstr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['len'].hist()\ntest['len'].hist()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = {'albert':(AlbertModel, AlbertTokenizer,\"albert-base-v2\"),\n        'roberta':(RobertaModel, RobertaTokenizer,'roberta-base')}\nmodel_name, model_tokenizer, pretrain = model['roberta']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''tokenizer albert'''\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\ntokenize = model_tokenizer.from_pretrained(pretrain)\ntoken = [tokenize.tokenize(t) for t in df['text']]\nids = [tokenize.convert_tokens_to_ids(t) for t in token]\nids = [tokenize.build_inputs_with_special_tokens(id) for id in ids]\nids = pad_sequences(ids, maxlen=24, truncating='post',dtype='long', padding='post')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['token'] = ids.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def mask(t):\n    mask = []\n    for i in t:\n        if i >0:\n            mask.append(1)\n        else:\n            mask.append(0)\n    return mask\ndf['mask'] = df['token'].apply(mask)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['mask'] = test['token'].apply(mask)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain , valid = train_test_split(df, test_size = 0.15, random_state = 345026)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nx_train = torch.tensor([np.array(i) for i in train['token']])\nx_mask = torch.tensor([np.array(i) for i in train['mask']])\nx_label =  torch.tensor(train['target'].values)\n\nx_val = torch.tensor([np.array(i) for i in valid['token']])\nx_maskvalid = torch.tensor([np.array(i) for i in valid['mask']])\nx_valid =  torch.tensor(valid['target'].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_all(seed_value):\n    random.seed(seed_value) # Python\n    np.random.seed(seed_value) # cpu vars\n    torch.manual_seed(seed_value) # cpu  vars\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value) # gpu vars\n        torch.backends.cudnn.deterministic = True  #needed\n        torch.backends.cudnn.benchmark = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import TensorDataset, RandomSampler, DataLoader\nimport random\nseed_all(123)\ntrain_tensor = TensorDataset(x_train, x_mask, x_label)\nsample = RandomSampler(train_tensor)\ntrain_loader = DataLoader(train_tensor, sampler=sample, batch_size=8)\n\nvalid_tensor = TensorDataset(x_val, x_maskvalid, x_valid)\nvalid_sample = RandomSampler(valid_tensor)\nvalid_loader = DataLoader(valid_tensor, sampler=valid_sample, batch_size=8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test = torch.tensor([np.array(i) for i in test['token']])\ntest_mask = torch.tensor([np.array(i) for i in test['mask']])\ntest_tensor = TensorDataset(x_test, test_mask)\nsample = RandomSampler(test_tensor)\ntest_loader = DataLoader(test_tensor, sampler=sample, batch_size=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class AlbertClassification(torch.nn.Module):\n    def __init__(self, numbers):\n        super(AlbertClassification,self).__init__()\n        self.model = model_name.from_pretrained(pretrain)\n        self.dense = torch.nn.Linear(768,numbers)\n        #self.dropout = torch.nn.Dropout(0.3)\n        torch.nn.init.xavier_normal_(self.dense.weight)\n\n    def forward(self, ids , attention_mask=None,token_type_ids=None):\n        last_hidden_state = self.model(input_ids=ids, attention_mask = attention_mask,token_type_ids= token_type_ids)\n        mean_last_hidden_state = self.pool_hidden_state(last_hidden_state)\n        #mean_last_hidden_state = self.dropout(mean_last_hidden_state)\n        logits = torch.sigmoid(self.dense(mean_last_hidden_state))\n        return logits\n    def frezze_parameters(self, index = 0):\n        if index:\n            self.model.parameters[index].requires_grad = False\n        else:\n            for param in self.model.parameters():\n                param.requires_grad = False\n    def unfree_parameters(self, index):\n        if index:\n            self.model.parameters[index].requires_grad = True\n        else:\n            for param in self.model.parameters():\n                param.requires_grad = True\n    def pool_hidden_state(self, last_hidden_state):\n        last_hidden_state = last_hidden_state[0]\n        mean_last_hidden_state = torch.mean(last_hidden_state, 1)\n        return mean_last_hidden_state\n    \nmodel = AlbertClassification(1)        \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optim = torch.optim.AdamW(model.parameters(),lr=2e-5, weight_decay=0.01)\nloss_f = torch.nn.BCEWithLogitsLoss()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\nfrom torch.autograd import Variable\n\ndef fit_cycle( model,epochs, train_loader, valid_loader,loss_f, optim,device = 'cpu'):\n    model.to(device)\n    model.train()\n    train_loss_set = []\n    for i in range(epochs):\n        print(\"epochs: \", i )\n        train_loss = 0\n        num_set = 0\n        for batch in tqdm(train_loader):            \n            x_train, x_mask, x_label = batch\n            x_train = Variable(x_train).cuda(device) #move tensor to cuda\n            x_mask = Variable(x_mask).cuda(device) #move tensor to cuda\n            x_label = Variable(x_label).cuda(device) #move tensor to cuda\n            optim.zero_grad()\n            ypred = model(x_train, attention_mask = x_mask)\n            loss = loss_f(ypred.reshape(-1), x_label.float())\n            train_loss += loss\n            num_set += x_train.shape[0]\n            loss.backward()\n            optim.step()\n            train_loss_set.append(float(train_loss/num_set))\n    print(\"Training loss is : \",train_loss)\n    model.eval()\n    valid_loss = 0\n    valid_loss_set = []\n    num_valid = 0\n    for batch in tqdm(valid_loader):            \n            x_train, x_mask, x_label = batch\n            x_train = Variable(x_train).cuda(device) #move tensor to cuda\n            x_mask = Variable(x_mask).cuda(device) #move tensor to cuda\n            x_label = Variable(x_label).cuda(device) #move tensor to cuda\n            with torch.no_grad():\n                ypred = model(x_train, attention_mask = x_mask)\n                loss = loss_f(ypred.reshape(-1), x_label.float())\n                train_loss += loss\n                num_valid += x_train.shape[0]\n            valid_loss_set.append(float(train_loss/num_valid))\n    print(\"Valid loss is : \", valid_loss)\n    return model , train_loss_set, valid_loss_set\n            \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"use_cuda = torch.cuda.is_available()\ndevice = torch.device('cuda:0' if use_cuda else 'cpu')\nprint(use_cuda)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmodel , train_loss, valid_loss = fit_cycle(model, 1, train_loader, valid_loader, loss_f, optim,device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(model, loader):\n    predict = []\n    for batch in tqdm(loader):\n        x_train, x_mask = batch\n        x_train = Variable(x_train).cuda(device) #move tensor to cuda\n        x_mask = Variable(x_mask).cuda(device) #move tensor to cuda\n        with torch.no_grad():\n            opt = model(x_train, attention_mask = x_mask)\n            predict.append(opt)\n    return predict\nypred = predict(model, test_loader)\npred = []\nfor i in ypred:\n    pred.append(torch.Tensor.cpu(i))\nplt.hist(pred, bins=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nplt.style.use('ggplot')\nplt.plot(train_loss)\nplt.plot(valid_loss)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}