{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import zipfile\nf = ['train', 'test']\nfor i in f:\n    with zipfile.ZipFile(\"/kaggle/input/sentiment-analysis-on-movie-reviews/\" + str(i)+ \".tsv.zip\",\"r\") as zip_ref:\n        zip_ref.extractall(\"/kaggle/working\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/working/train.tsv\", sep='\\t')\ntest = pd.read_csv(\"/kaggle/working/test.tsv\", sep='\\t')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[['Phrase','Sentiment']]\ndf.columns = ['text', 'label']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = test[['Phrase']]\ntest.columns = ['text']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#preprocessing data.\n#remove punc, html, stop works,\nfrom bs4 import BeautifulSoup\nfrom nltk.tokenize import word_tokenize\n\nimport re\nfrom tqdm import tqdm\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\ndef clean_sentences(df):\n    reviews = []\n\n    for sent in tqdm(df['text']):\n        \n        #remove html content\n        review_text = BeautifulSoup(sent).get_text()\n        \n        #remove non-alphabetic characters\n        review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n    \n        #tokenize the sentences\n        words = word_tokenize(review_text.lower())\n    \n        #lemmatize each word to its lemma\n        lemma_words = [lemmatizer.lemmatize(i) for i in words]\n    \n        reviews.append(\" \".join(lemma_words))\n\n    return(reviews)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text'] = clean_sentences(df)\ntest['text'] = clean_sentences(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text'][:20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['text'][:20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def strl(t):\n    return len(t.split())\ndf['len'] = df['text'].apply(strl)\ntest['len'] = test['text'].apply(strl)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['len'].hist()\ntest['len'].hist()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# preprocessing data with the same input  ---> same output\nps = df['text'].value_counts()\nvalue = list(ps[ps>2].index)\nfor key in value:\n    ls = df[df['text']==key]['label'].value_counts().index[0]\n    df.loc[df['text'] ==key, 'label'] = ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['label'].hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df['text']=='']['label'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom transformers import *\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"XLMRobertaForSequenceClassification.pretrained_model_archive_map.keys()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MODEL_CLASS = {'distilbert': (DistilBertModel, DistilBertTokenizer, 'distilbert-base'),\n              'xlmroberta':(XLMRobertaModel, XLMRobertaTokenizer, 'xlm-roberta-base'),\n              'roberta': (RobertaModel, RobertaTokenizer, 'roberta-base'),\n              'xlm':(XLMModel,XLMTokenizer,'xlm-mlm-en-2048'),\n              'albert':(AlbertModel, AlbertTokenizer, 'albert-base-v2')}\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#parameter \nbs = 16\nfpt16 = False\nseed = 345026\nmodel_name = 'roberta'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\ndef seed_all(seed_value):\n    random.seed(seed_value) # Python\n    np.random.seed(seed_value) # cpu vars\n    torch.manual_seed(seed_value) # cpu  vars\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value) # gpu vars\n        torch.backends.cudnn.deterministic = True  #needed\n        torch.backends.cudnn.benchmark = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed_all(seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_name , model_tokenizer, pretrain = MODEL_CLASS[model_name]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.sequence import pad_sequences\ndef tokenize(df,tokenizer, pretrain):\n    tokenizer = tokenizer.from_pretrained(pretrain)\n    token = [tokenizer.tokenize(t) for t in df.text]\n    ids = [tokenizer.convert_tokens_to_ids(t) for t in token]\n    ids = [tokenizer.build_inputs_with_special_tokens(t) for t in ids]\n    df['token'] = pad_sequences(ids, maxlen =64, truncating='post', dtype='long',padding='post').tolist()\n    return df\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = tokenize(df, model_tokenizer,pretrain)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = tokenize(test, model_tokenizer, pretrain)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def mask_create(t):\n    mask = []\n    for i in t:\n        if i>0:\n            mask.append(1)\n        else:\n            mask.append(0)\n    return mask\ntest['mask']=test['token'].apply(mask_create)\ndf['mask'] = df['token'].apply(mask_create)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain ,valid = train_test_split(df, random_state=seed, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model defind\nfrom keras.utils import to_categorical\ntrain_x  = torch.tensor([np.array(t) for t in train['token']])\ntrain_mask = torch.tensor([np.array(t) for t in train['mask']], dtype=torch.long)\ntrain_y = torch.tensor(to_categorical(train['label'].values), dtype=torch.long)\nvalid_x = torch.tensor([np.array(t) for t in valid['token']])\nvalid_mask = torch.tensor([np.array(i) for i in valid['mask']], dtype=torch.long)\nvalid_y = torch.tensor(to_categorical(valid['label'].values), dtype=torch.long)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_x = torch.tensor([np.array(t) for t in test['token']])\ntest_mask = torch.tensor([np.array(i) for i in test['mask']], dtype=torch.long)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import TensorDataset, RandomSampler, DataLoader\n\ntrain_tensor = TensorDataset(train_x, train_mask, train_y)\nsample = RandomSampler(train_tensor)\ntrainloader = DataLoader(train_tensor ,sampler=sample, batch_size= bs)\n\nvalid_tensor = TensorDataset(valid_x, valid_mask, valid_y)\nsample = RandomSampler(valid_tensor)\nvalidloader = DataLoader(valid_tensor ,sampler=sample, batch_size= bs)\n\ntest_tensor = TensorDataset(test_x, test_mask)\nsample = RandomSampler(test_tensor)\ntestloader = DataLoader(test_tensor ,sampler=sample, batch_size= 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#deploy model with 2 layer : roberta and dense\n\nclass RobertaMultilayerClassification(torch.nn.Module):\n    def __init__(self,model,pretrain,out_layers=1):\n        super(RobertaMultilayerClassification,self).__init__()\n        self.model = model.from_pretrained(pretrain)\n        self.dense = torch.nn.Linear(768, out_layers)\n        torch.nn.init.xavier_normal_(self.dense.weight)\n\n    def forward(self, ids , attention_mask=None,token_type_ids=None):\n        last_hidden_state = self.model(input_ids=ids, attention_mask = attention_mask,token_type_ids= token_type_ids)\n        mean_last_hidden_state = self.pool_hidden_state(last_hidden_state)\n        #mean_last_hidden_state = self.dropout(mean_last_hidden_state)\n        logits = self.dense(mean_last_hidden_state)\n        return logits\n    def frezze_parameters(self, index = 0):\n        if index:\n            self.model.parameters[index].requires_grad = False\n        else:\n            for param in self.model.parameters():\n                param.requires_grad = False\n    def unfree_parameters(self, index):\n        if index:\n            self.model.parameters[index].requires_grad = True\n        else:\n            for param in self.model.parameters():\n                param.requires_grad = True\n    def pool_hidden_state(self, last_hidden_state):\n        last_hidden_state = last_hidden_state[0]\n        #mean_last_hidden_state = torch.mean(last_hidden_state, 1)\n        #return mean_last_hidden_state\n        return last_hidden_state\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = len(df['label'].unique())\nroberta = RobertaMultilayerClassification(model_name,pretrain,labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roberta.model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\noptim = torch.optim.AdamW(params=roberta.parameters(), lr=2e-5,weight_decay=0.01)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\nfrom torch.autograd import Variable\ndef fit_cycle( model, epochs, optim , train_loader , valid_loader, device):\n    model.train()\n    model.to(device)\n\n    for i in range(epochs):\n        loss_set = []\n        steps = 0\n        loss_sum  = 0\n        for batch in tqdm(train_loader):\n            x_train ,x_mask, y_train = batch\n            x_train = Variable(x_train).cuda(device)\n            x_mask  = Variable(x_mask).cuda(device)\n            y_train = Variable(y_train).cuda(device)\n            ypred = model(x_train, attention_mask=x_mask)\n            optim.zero_grad()\n            loss = torch.nn.BCEWithLogitsLoss(ypred,y_train)\n            loss_sum += loss\n            steps +=y_train.shape[0]\n            loss.backward()\n            optim.step()\n            loss_set.append(loss_sum/steps)\n    model.eval()\n    valid_set = []\n    steps = 0\n    loss_sum  = 0\n    for batch in tqdm(valid_loader):\n            x_train ,x_mask, y_train = batch\n            x_train = Variable(x_train).cuda(device)\n            x_mask  = Variable(x_mask).cuda(device)\n            y_train = Variable(y_train).cuda(device)\n            ypred = model(x_train, attention_mask=x_mask)\n            with torch.no_grad():\n                loss = torch.nn.BCEWithLogitsLoss(ypred,y_train)\n                loss_sum += loss\n                steps +=y_train.shape[0]\n                valid_set.append(loss_sum/steps)\n    return model,loss_set,valid_set\n        \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device('cuda:0' if use_cuda else 'cpu')\nprint(use_cuda)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roberta, loss, valid = fit_cycle(roberta, 1,optim, trainloader, validloader, device = device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(loss)\nplt.plot(valid)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(model, loader):\n    model.to('cuda')\n    predict = []\n    for batch in tqdm(loader):\n        x_train, x_mask = batch\n        x_train = Variable(x_train).cuda(device) #move tensor to cuda\n        x_mask = Variable(x_mask).cuda(device) #move tensor to cuda\n        with torch.no_grad():\n            opt = model(x_train, attention_mask = x_mask)\n            predict.append(opt)\n    return predict\nypred = predict(roberta, testloader)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = []\nfor i in ypred:\n    pred.append(np.array(torch.Tensor.cpu(i)))\npred = np.array(pred)\npred = pred.reshape(pred.shape[0],-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(np.argmax(pred, axis=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ypred[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}